---
title: "Weakly Supervised Learning: Label Noise and Correction"
subtitle: "Applied Data Science Project 3"
author: "Marcus Loke"
date: "Spring 2022"
output:
  html_document: 
    theme: lumen
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
# Load required libraries
library(reticulate)
library(keras) # high-level neural networks API, written in Python and capable of running on top of TensorFlow
library(tensorflow)
library(nnet)
library(dplyr)
library(png)
library(glue)
library(stringr)
```

## Load the datasets

For the project, we provide a training set with 50,000 images in the directory `../data/images/` with:

+ noisy labels for all images provided in `../data/noisy_labels.csv`;
+ clean labels for the first 10,000 images provided in `../data/clean_labels.csv`.

```{r}
# Load the images
n_img = 50000
n_noisy = 40000
n_clean_noisy = n_img - n_noisy
imgs = array(NA, c(n_img,32,32,3))

for (i in 1:n_img){
  img_fn = glue("../data/images/{str_pad(i, 5, pad='0')}.png")
  imgs[i,,,] = readPNG(img_fn) 
  i = i + 1
}

# Load the labels
clean_labels = as.matrix(read.csv("../data/clean_labels.csv", header=F))
noisy_labels = as.matrix(read.csv("../data/noisy_labels.csv", header=F))

# Combine into lists
cifar = list()
cifar$train$x = imgs
cifar$train$y = noisy_labels
cifar$test$x = imgs[1:10000,,,]
cifar$test$y = clean_labels
```

### Verify the data

For illustration, we present a small subset (of size 8) of the images with their clean and noisy labels in `clean_noisy_trainset`. You are encouraged to explore more characteristics of the label noises on the whole dataset.

```{r}
# The class label correspondence
classes = c('plane', 'car', 'bird', 'cat', 
            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

index = 1:8

# Print clean labels
par(mfrow = c(2,4), mar = rep(1, 4), oma = rep(0.2, 4))
cifar$train$x[index,,,] %>% 
  purrr::array_tree(1) %>%
  purrr::set_names(classes[clean_labels[index] + 1]) %>% 
  purrr::map(as.raster, max = 1) %>%
  purrr::iwalk(~{plot(.x); title(.y)})

# Print noisy labels
par(mfrow = c(2,4), mar = rep(1, 4), oma = rep(0.2, 4))
cifar$train$x[index,,,] %>% 
  purrr::array_tree(1) %>%
  purrr::set_names(classes[noisy_labels[index] + 1]) %>% 
  purrr::map(as.raster, max = 1) %>%
  purrr::iwalk(~{plot(.x); title(.y)})
```

## The predictive model

We consider a baseline model directly on the noisy dataset without any label corrections. RGB histogram features are extracted to fit a logistic regression model.

### Baseline model

```{r}
# RGB histogram data construction
no_bins = 6
bins = array(seq(0, 1, length.out=no_bins)) # the range of the rgb histogram
target_vec = array(NA, n_img)
feature_mtx = array(NA, c(n_img,3*(length(bins)-1)))
i = 0

for (i in 1:n_img){
  # The target vector consists of noisy labels
  target_vec[i] = noisy_labels[i]
  
  # Use the number of pixels in each bin for all three channels as features
  feature1 = array(hist(imgs[i,,,1], breaks = bins, plot = F)$counts)
  feature2 = array(hist(imgs[i,,,2], breaks = bins, plot = F)$counts)
  feature3 = array(hist(imgs[i,,,3], breaks = bins, plot = F)$counts)
  
  # Concatenate three features
  feature_mtx[i,] = c(feature1, feature2, feature3)
  i = i + 1
}
```

```{r, message=FALSE}
# Train a multinomial logistic regression model with last 40,000 images
df_feature_mtx = cbind(as.data.frame(feature_mtx[10001:50000,]),
                       as.data.frame(target_vec[10001:50000])) %>%
  rename(target_vec = `target_vec[10001:50000]`)
clf = multinom(target_vec~., data = df_feature_mtx)

# Extract features for first 10,000 images (clean labels)
feature = array(NA, c(n_clean_noisy,3*(length(bins)-1)))
i = 0
for (i in 1:n_clean_noisy){
  # Use the number of pixels in each bin for all three channels as features
  feature1 = array(hist(imgs[i,,,1], breaks = bins, plot = F)$counts)
  feature2 = array(hist(imgs[i,,,2], breaks = bins, plot = F)$counts)
  feature3 = array(hist(imgs[i,,,3], breaks = bins, plot = F)$counts)
  
  # Concatenate three features
  feature[i,] = c(feature1, feature2, feature3)
  i = i + 1
}

# Predict on first 10,000 images and show accuracy
sum(as.integer(predict(clf, as.data.frame(feature))) == as.data.frame(clean_labels)) / 10000
```

For the convenience of evaluation, we write the following function `predictive_model` that does the label prediction. For your predictive model, feel free to modify the function, but make sure the function takes an RGB image of format with dimension $32 \times 32 \times 3$ as input, and returns one single label as output.

```{r}

```

### Model 1

Here we build a convolutional neural network (CNN) that's trained on the last 40,000 images with noisy labels (i.e., image #10,001 to #50,000) and tested on the first 10,000 images with clean labels (i.e., image #1 to #10,000). This way of segmenting the data into train and validation sets are in the proportion of 80% to 20% respectively.

```{r, message=FALSE}
# Sequential = layer by layer building; not possible to share layers
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "relu",
                input_shape = c(32,32,3)) %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu")

# Now flatten and apply neural net calculation
model %>%
  layer_flatten() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 10, activation = "softmax")

# Compile and train model
model %>% 
  compile(
    optimizer = "adam",
    loss = "sparse_categorical_crossentropy",
    metrics = "accuracy"
  )

# Fit the model
start_time = Sys.time()
results_train <- model %>% 
  fit(
    x = cifar$train$x[10001:50000,,,], y = cifar$train$y[10001:50000],
    epochs = 10,
    validation_data = unname(cifar$test),
    verbose = 2
  )
end_time = Sys.time()

# Evaluate model
plot(results_train)
evaluate(model, cifar$test$x, cifar$test$y, verbose = 0)

# Time taken to train model
end_time - start_time
```

### Model 2

## Evaluation

```{r}

```